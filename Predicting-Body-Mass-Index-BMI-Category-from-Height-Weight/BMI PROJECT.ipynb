{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78970c8",
   "metadata": {},
   "source": [
    "# Machine Learning Project: Predicting BMI Categories from Height & Weight\n",
    "\n",
    "## Project Overview\n",
    "This project aims to predict Body Mass Index (BMI) categories using machine learning techniques based on height and weight measurements. We'll explore the dataset, perform data preprocessing, train multiple classification models, and evaluate their performance.\n",
    "\n",
    "### BMI Categories:\n",
    "- **Underweight**: BMI < 18.5\n",
    "- **Normal weight**: 18.5 ≤ BMI < 25\n",
    "- **Overweight**: 25 ≤ BMI < 30\n",
    "- **Obese**: BMI ≥ 30\n",
    "\n",
    "### Dataset Features:\n",
    "- Sex (Male/Female)\n",
    "- Age\n",
    "- Height (in inches)\n",
    "- Weight (in pounds)\n",
    "- BMI (target for category prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c30051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Python version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e7224",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Upload\n",
    "\n",
    "**For Google Colab Users:**\n",
    "1. Upload your `bmi_data.csv` file using the file upload feature\n",
    "2. Run the cell below to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Upload the dataset\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the BMI dataset\n",
    "print(\"Please upload your bmi_data.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('bmi_data.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809987d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ec7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb17b08",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc944cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Before cleaning:\", df_clean.shape)\n",
    "\n",
    "# Drop rows with missing BMI values (our target variable)\n",
    "df_clean = df_clean.dropna(subset=['BMI'])\n",
    "\n",
    "# For other missing values, we'll use median imputation\n",
    "df_clean['Height(Inches)'].fillna(df_clean['Height(Inches)'].median(), inplace=True)\n",
    "df_clean['Weight(Pounds)'].fillna(df_clean['Weight(Pounds)'].median(), inplace=True)\n",
    "df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)\n",
    "\n",
    "print(\"After cleaning:\", df_clean.shape)\n",
    "\n",
    "# Create BMI categories\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif 18.5 <= bmi < 25:\n",
    "        return 'Normal weight'\n",
    "    elif 25 <= bmi < 30:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obese'\n",
    "\n",
    "df_clean['BMI_Category'] = df_clean['BMI'].apply(categorize_bmi)\n",
    "\n",
    "print(\"\\nBMI Category Distribution:\")\n",
    "print(df_clean['BMI_Category'].value_counts())\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print(df_clean['BMI_Category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BMI category distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# BMI Category Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "df_clean['BMI_Category'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('BMI Category Distribution')\n",
    "plt.xlabel('BMI Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# BMI Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df_clean['BMI'], bins=30, color='lightcoral', alpha=0.7)\n",
    "plt.title('BMI Distribution')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# BMI by Gender\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=df_clean, x='Sex', y='BMI')\n",
    "plt.title('BMI Distribution by Gender')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create correlation heatmap\n",
    "numeric_cols = ['Age', 'Height(Inches)', 'Weight(Pounds)', 'BMI']\n",
    "correlation_matrix = df_clean[numeric_cols].corr()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Scatter plots\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(df_clean['Height(Inches)'], df_clean['Weight(Pounds)'], \n",
    "           c=df_clean['BMI'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='BMI')\n",
    "plt.xlabel('Height (Inches)')\n",
    "plt.ylabel('Weight (Pounds)')\n",
    "plt.title('Height vs Weight (colored by BMI)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_clean['Height(Inches)'], df_clean['BMI'], alpha=0.6, color='orange')\n",
    "plt.xlabel('Height (Inches)')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Height vs BMI')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(df_clean['Weight(Pounds)'], df_clean['BMI'], alpha=0.6, color='green')\n",
    "plt.xlabel('Weight (Pounds)')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Weight vs BMI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9433f3f",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9437c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for machine learning\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "df_clean['Sex_encoded'] = label_encoder.fit_transform(df_clean['Sex'])\n",
    "\n",
    "# Select features (X) and target (y)\n",
    "X = df_clean[['Sex_encoded', 'Age', 'Height(Inches)', 'Weight(Pounds)']]\n",
    "y = df_clean['BMI_Category']\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.head())\n",
    "print(f\"\\nFeature shapes: {X.shape}\")\n",
    "\n",
    "print(\"\\nTarget (y):\")\n",
    "print(y.head())\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Encode target variable for machine learning\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nTarget classes: {target_encoder.classes_}\")\n",
    "print(f\"Encoded target sample: {y_encoded[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling completed!\")\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ae677",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c84272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "print(\"Training and evaluating models:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    if name == 'SVM':\n",
    "        # Use scaled features for SVM\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # Use original features for tree-based models\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    if name == 'SVM':\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{name} CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Comparison Summary:\")\n",
    "for name, results in model_results.items():\n",
    "    print(f\"{name:20} | Accuracy: {results['accuracy']:.4f} | CV: {results['cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d7780",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Model accuracy comparison\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "cv_scores = [model_results[name]['cv_mean'] for name in model_names]\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "x_pos = np.arange(len(model_names))\n",
    "plt.bar(x_pos, accuracies, color='skyblue', alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(x_pos, model_names, rotation=45, ha='right')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(x_pos, cv_scores, color='lightcoral', alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('CV Score')\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.xticks(x_pos, model_names, rotation=45, ha='right')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add CV scores on bars\n",
    "for i, cv in enumerate(cv_scores):\n",
    "    plt.text(i, cv + 0.01, f'{cv:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best performing model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Best accuracy: {model_results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "# Classification report\n",
    "print(f\"Detailed Classification Report for {best_model_name}:\")\n",
    "print(\"=\"*70)\n",
    "class_names = target_encoder.classes_\n",
    "print(classification_report(y_test, best_predictions, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = confusion_matrix(y_test, best_predictions, normalize='true')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Normalized Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (for tree-based models)\n",
    "if best_model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "    best_model = models[best_model_name]\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = ['Sex', 'Age', 'Height(Inches)', 'Weight(Pounds)']\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"Feature Importance for {best_model_name}:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    \n",
    "    # Feature importance pie chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(importance_df['importance'], labels=importance_df['feature'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Feature Importance Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d3f71",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "print(f\"Performing hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    # Random Forest hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    model_for_tuning = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    # Gradient Boosting hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    model_for_tuning = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "elif best_model_name == 'SVM':\n",
    "    # SVM hyperparameters\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "    model_for_tuning = SVC(random_state=42)\n",
    "    \n",
    "else:\n",
    "    # Default parameters for other models\n",
    "    param_grid = {}\n",
    "    model_for_tuning = models[best_model_name]\n",
    "\n",
    "# Perform Grid Search\n",
    "if param_grid:\n",
    "    print(\"Performing Grid Search...\")\n",
    "    if best_model_name == 'SVM':\n",
    "        grid_search = GridSearchCV(model_for_tuning, param_grid, cv=3, \n",
    "                                 scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(model_for_tuning, param_grid, cv=3, \n",
    "                                 scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    if best_model_name == 'SVM':\n",
    "        tuned_predictions = grid_search.predict(X_test_scaled)\n",
    "    else:\n",
    "        tuned_predictions = grid_search.predict(X_test)\n",
    "    \n",
    "    tuned_accuracy = accuracy_score(y_test, tuned_predictions)\n",
    "    print(f\"Tuned model accuracy: {tuned_accuracy:.4f}\")\n",
    "    print(f\"Improvement: {tuned_accuracy - model_results[best_model_name]['accuracy']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"No hyperparameter tuning performed for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcad9e",
   "metadata": {},
   "source": [
    "## 8. Making Predictions with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict BMI category for new individuals\n",
    "def predict_bmi_category(sex, age, height_inches, weight_pounds, model_name=best_model_name):\n",
    "    \"\"\"\n",
    "    Predict BMI category for a new individual\n",
    "    \n",
    "    Parameters:\n",
    "    - sex: 'Male' or 'Female'\n",
    "    - age: Age in years\n",
    "    - height_inches: Height in inches\n",
    "    - weight_pounds: Weight in pounds\n",
    "    - model_name: Name of the model to use for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode sex\n",
    "    sex_encoded = 1 if sex.lower() == 'male' else 0\n",
    "    \n",
    "    # Create feature array\n",
    "    features = np.array([[sex_encoded, age, height_inches, weight_pounds]])\n",
    "    \n",
    "    # Get the model\n",
    "    if 'grid_search' in locals() and param_grid:\n",
    "        # Use tuned model if available\n",
    "        model = grid_search\n",
    "        if model_name == 'SVM':\n",
    "            features_scaled = scaler.transform(features)\n",
    "            prediction = model.predict(features_scaled)[0]\n",
    "        else:\n",
    "            prediction = model.predict(features)[0]\n",
    "    else:\n",
    "        # Use original model\n",
    "        model = models[model_name]\n",
    "        if model_name == 'SVM':\n",
    "            features_scaled = scaler.transform(features)\n",
    "            prediction = model.predict(features_scaled)[0]\n",
    "        else:\n",
    "            prediction = model.predict(features)[0]\n",
    "    \n",
    "    # Convert prediction back to category name\n",
    "    category = target_encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    # Calculate actual BMI for comparison\n",
    "    actual_bmi = (weight_pounds / (height_inches ** 2)) * 703\n",
    "    \n",
    "    return {\n",
    "        'predicted_category': category,\n",
    "        'actual_bmi': round(actual_bmi, 2),\n",
    "        'actual_category': categorize_bmi(actual_bmi)\n",
    "    }\n",
    "\n",
    "# Test the prediction function with sample data\n",
    "print(\"Testing Prediction Function:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample predictions\n",
    "test_cases = [\n",
    "    ('Female', 25, 65, 120),  # Expected: Normal weight\n",
    "    ('Male', 30, 72, 200),    # Expected: Overweight/Obese\n",
    "    ('Female', 22, 62, 100),  # Expected: Underweight\n",
    "    ('Male', 35, 70, 180)     # Expected: Normal weight/Overweight\n",
    "]\n",
    "\n",
    "for i, (sex, age, height, weight) in enumerate(test_cases, 1):\n",
    "    result = predict_bmi_category(sex, age, height, weight)\n",
    "    print(f\"\\nTest Case {i}:\")\n",
    "    print(f\"Input: {sex}, Age: {age}, Height: {height}\\\", Weight: {weight} lbs\")\n",
    "    print(f\"Actual BMI: {result['actual_bmi']}\")\n",
    "    print(f\"Actual Category: {result['actual_category']}\")\n",
    "    print(f\"Predicted Category: {result['predicted_category']}\")\n",
    "    print(f\"Prediction Correct: {result['actual_category'] == result['predicted_category']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction (uncomment to use)\n",
    "\"\"\"\n",
    "print(\"\\\\nInteractive BMI Category Prediction\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get user input\n",
    "sex = input(\"Enter sex (Male/Female): \")\n",
    "age = int(input(\"Enter age: \"))\n",
    "height = float(input(\"Enter height in inches: \"))\n",
    "weight = float(input(\"Enter weight in pounds: \"))\n",
    "\n",
    "# Make prediction\n",
    "result = predict_bmi_category(sex, age, height, weight)\n",
    "\n",
    "print(f\"\\\\n🏥 BMI Prediction Results:\")\n",
    "print(f\"📊 Calculated BMI: {result['actual_bmi']}\")\n",
    "print(f\"🎯 Actual Category: {result['actual_category']}\")\n",
    "print(f\"🤖 AI Predicted Category: {result['predicted_category']}\")\n",
    "\n",
    "if result['actual_category'] == result['predicted_category']:\n",
    "    print(\"✅ The AI prediction matches the calculated BMI category!\")\n",
    "else:\n",
    "    print(\"⚠️  The AI prediction differs from the calculated BMI category.\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Interactive prediction section ready!\")\n",
    "print(\"Uncomment the code above to enable interactive predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa0f04",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: The machine learning models successfully predict BMI categories with high accuracy\n",
    "2. **Feature Importance**: Weight and height are typically the most important features (as expected)\n",
    "3. **Model Comparison**: Different algorithms show varying performance levels\n",
    "4. **Data Quality**: Proper data cleaning and preprocessing significantly impact model performance\n",
    "\n",
    "### Business Applications:\n",
    "\n",
    "1. **Healthcare Screening**: Quick BMI category assessment for large populations\n",
    "2. **Health Apps**: Integration into fitness and health monitoring applications\n",
    "3. **Medical Research**: Population health studies and trend analysis\n",
    "4. **Insurance**: Risk assessment and premium calculations\n",
    "\n",
    "### Model Limitations:\n",
    "\n",
    "1. **BMI Limitations**: BMI doesn't account for muscle mass vs fat mass\n",
    "2. **Demographic Factors**: Model may need adjustment for different populations\n",
    "3. **Data Dependencies**: Performance depends on quality and representativeness of training data\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Additional Features**: Include body fat percentage, muscle mass, ethnicity\n",
    "2. **Advanced Models**: Experiment with neural networks or ensemble methods\n",
    "3. **Real-time Learning**: Implement online learning for continuous model updates\n",
    "4. **Integration**: Connect with wearable devices for automatic data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Summary\n",
    "print(\"🎯 BMI CATEGORY PREDICTION PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 Dataset Size: {df_clean.shape[0]:,} samples\")\n",
    "print(f\"🔧 Features Used: {list(X.columns)}\")\n",
    "print(f\"🏆 Best Model: {best_model_name}\")\n",
    "print(f\"📈 Best Accuracy: {model_results[best_model_name]['accuracy']:.1%}\")\n",
    "print(f\"🎨 BMI Categories: {list(target_encoder.classes_)}\")\n",
    "print(\"\\n✅ Project completed successfully!\")\n",
    "print(\"\\n📋 Next Steps:\")\n",
    "print(\"1. Deploy the model to a web application\")\n",
    "print(\"2. Collect more diverse data for better generalization\")\n",
    "print(\"3. Implement real-time prediction API\")\n",
    "print(\"4. Add model monitoring and retraining pipeline\")\n",
    "print(\"5. Integrate with healthcare systems\")\n",
    "\n",
    "# Save model summary\n",
    "model_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'accuracy': model_results[best_model_name]['accuracy'],\n",
    "    'dataset_size': df_clean.shape[0],\n",
    "    'features': list(X.columns),\n",
    "    'categories': list(target_encoder.classes_)\n",
    "}\n",
    "\n",
    "print(f\"\\n🔍 Model ready for deployment and predictions!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
